{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA and Preprocessing Walkthrough\n",
        "\n",
        "This notebook explores the synthetic hospital readmission dataset, performs exploratory data analysis, and documents the preprocessing pipeline aligned with the AI Development Workflow assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "DATA_PATH = Path(\"../data/synthetic_patients.csv\")\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Summary\n",
        "\n",
        "The dataset contains demographic, clinical, and social determinants features. The target column `readmitted` indicates whether the patient returned within 30 days of discharge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "df.describe(include=\"all\").transpose()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "class_counts = df['readmitted'].value_counts().rename({0: 'No Readmit', 1: 'Readmit'})\n",
        "class_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
        "plt.title(\"Readmission Class Distribution\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing Steps\n",
        "\n",
        "See `src/data_pipeline.py` for the full column transformer and SMOTE pipeline. Key operations:\n",
        "\n",
        "1. Impute missing values (median for numeric, most frequent for categorical).\n",
        "2. Scale numeric features and one-hot encode categoricals.\n",
        "3. Vectorize discharge summaries with TF-IDF (bigrams, max 256 features).\n",
        "4. Apply SMOTE to balance readmission classes.\n",
        "\n",
        "The following cell illustrates how to instantiate the preprocessing pipeline directly from the configuration file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from src.utils import load_config\n",
        "from src.data_pipeline import prepare_datasets\n",
        "\n",
        "config = load_config(\"../config/experiment.yaml\")\n",
        "dataset_splits, preprocessing_pipeline = prepare_datasets(config)\n",
        "preprocessing_pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- Run `src/train_model.py` to train LightGBM with the processed features.\n",
        "- Review `docs/ai_workflow_report.md` for the comprehensive narrative.\n",
        "- Publish synthesized insights to the PLP Academy Community using `docs/plp_article_post.md` as a template.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
